# -*- coding: utf-8 -*-
"""Quantile_Gradient_Boosting_for_Probabilistic_VTEC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RsXFl32QPHNOheMrRgwDOQiB6j-iFyyb

This notebook demonstrates how to load and evaluate  the probabilistic Quantile Gradient Boosting (QGB) Vertical Total Electron Content (VTEC) models, which provide 95% confidence intervals. QGB VTEC models forecast VTEC 1-day ahead for grid points at 10° of longitude, and 10°, 40°, and 70° of latitude. They were developed within the study "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M., submitted to the Space Weather Jornal, AGU. 

Quantiles were estimated by multiplying the quantile values β by the positive and negative residuals in the loss function to obtain the quantile loss (QL) (see Equation 7 in the paper). 
Quantile values of $\beta=\left \{0.025, 0.975 \right \}$ are chosen for estimating the lower and upper confidence bounds, respectively, to obtain a confidence interval of $95\%$. The mean quantile $\beta=\left \{0.50 \right \}$ provides the median VTEC. 

It has been shown that the quantile loss can model the data uncertainty. The Gradient Boosted tree is fast, performs well on structured input data, even on relatively small datasets, and has proven to be a powerful method in many data science competitions. For more information on Gradient Boosting for VTEC, see Natras, R.; Soja, B.; Schmidt, M. Ensemble Machine Learning of Random Forest, AdaBoost and XGBoost for Vertical Total Electron Content Forecasting. Remote Sens. 2022, 14, 3547. https://doi.org/10.3390/rs14153547.

The notebook was created by Randa Natras: randa.natras@hotmail.com; randa.natras@tum.de

**Installation**
"""

#Installation of the version of Scikit-learn used for the training of the models
!pip install scikit-learn==1.0.2

"""### **Libraries**"""

import numpy as np
import pandas as pd
import datetime
import matplotlib.dates as mdates
from matplotlib.dates import DateFormatter
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats
from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib

"""## Loading the test dataset

The dataset from the paper "Uncertainty Quantification for Machine Learning-based Ionosphere and Space Weather Forecasting" by Natras R., Soja B. and Schmidt M.
"""

data_test_df= pd.read_csv ('Test_data_VTEC.csv', delimiter=';')

data_test_df.head()

#In the next lines, a date-time column is created and set as the index column.
data_test_df['Date-time'] = (pd.to_datetime(data_test_df['Year'] * 1000 + data_test_df['DOY'], format='%Y%j') + pd.to_timedelta(data_test_df['Hour '], unit='h'))
data_test_df_2 = data_test_df.set_index('Date-time')

data_test_df_2.head()

"""### Prepare input features"""

#Calculating the exponential moving average VTEC over 30 days.
data_test_df_2['VTEC_10N_EMA(30d)'] = data_test_df_2['VTEC_10N'].ewm(span=(30*24), adjust=False).mean()
data_test_df_2['VTEC_40N_EMA(30d)'] = data_test_df_2['VTEC_40N'].ewm(span=(30*24), adjust=False).mean()
data_test_df_2['VTEC_70N_EMA(30d)'] = data_test_df_2['VTEC_70N'].ewm(span=(30*24), adjust=False).mean()

#Calculating the exponential moving average VTEC over 4 days (96 hours).
data_test_df_2['VTEC_10N_EMA(96h)'] = data_test_df_2['VTEC_10N'].ewm(span=(4*24), adjust=False).mean()
data_test_df_2['VTEC_40N_EMA(96h)'] = data_test_df_2['VTEC_40N'].ewm(span=(4*24), adjust=False).mean()
data_test_df_2['VTEC_70N_EMA(96h)'] = data_test_df_2['VTEC_70N'].ewm(span=(4*24), adjust=False).mean()

#first order derivative
data_test_df_2['VTEC_10N_der1'] = data_test_df_2['VTEC_10N'].diff()
data_test_df_2['VTEC_40N_der1'] = data_test_df_2['VTEC_40N'].diff()
data_test_df_2['VTEC_70N_der1'] = data_test_df_2['VTEC_70N'].diff()

#second order derivative
data_test_df_2['VTEC_10N_der2'] = data_test_df_2['VTEC_10N'].diff().diff()
data_test_df_2['VTEC_40N_der2'] = data_test_df_2['VTEC_40N'].diff().diff()
data_test_df_2['VTEC_70N_der2'] = data_test_df_2['VTEC_70N'].diff().diff()

#Calculation of the new features leads to NaN values, which are now omitted
data_test_df_2.dropna(inplace = True)

data_test_df_2.head()

"""Learning algorithms based on decision trees do not require data normalization because they are not sensitive to the scale of input features. So the data were not standardized.

No sin/cos encoding is performed for features Day of year and Hour of day, because the decision tree does not use Euclidean distances, but works differently. It splits the dataset from one region into two non-overlapping sub-regions based on a single splitting variable and the split point (for more details, see https://www.mdpi.com/2072-4292/14/15/3547). Therefore, decision trees do not benefit from sin/cos encoding.

Preparation of the data for testing the model: extraction of the X and Y variables.
"""

X_10_test_df=data_test_df_2.drop (['Year','VTEC_40N', 'VTEC_70N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)', 'VTEC_40N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_70N_EMA(96h)',  'VTEC_40N_der1', 'VTEC_70N_der1', 'VTEC_40N_der2', 'VTEC_70N_der2' ], axis=1)
X_40_test_df=data_test_df_2.drop (['Year','VTEC_10N', 'VTEC_70N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' , 'VTEC_10N_EMA(30d)', 'VTEC_70N_EMA(30d)', 'VTEC_10N_EMA(96h)', 'VTEC_70N_EMA(96h)', 'VTEC_10N_der1', 'VTEC_70N_der1', 'VTEC_10N_der2', 'VTEC_70N_der2'], axis=1)
X_70_test_df=data_test_df_2.drop (['Year','VTEC_10N', 'VTEC_40N', 'VTEC_10N (t+24)', 'VTEC_40N (t+24)','VTEC_70N (t+24)' , 'VTEC_40N_EMA(30d)', 'VTEC_10N_EMA(30d)', 'VTEC_40N_EMA(96h)', 'VTEC_10N_EMA(96h)', 'VTEC_40N_der1', 'VTEC_10N_der1', 'VTEC_40N_der2', 'VTEC_10N_der2'], axis=1)

X_10_test_df.head()

X_10_test_new = X_10_test_df.to_numpy()
X_40_test_new = X_40_test_df.to_numpy()
X_70_test_new = X_70_test_df.to_numpy()

y_10_test_new = data_test_df_2 ['VTEC_10N (t+24)'].to_numpy()
y_40_test_new = data_test_df_2 ['VTEC_40N (t+24)'].to_numpy()
y_70_test_new = data_test_df_2 ['VTEC_70N (t+24)'].to_numpy()

"""# **Quantile Gradient Boosting (QGB) VTEC Models**

## **Loading the models**

Separate models were trained for each value of $\beta= \{0.025, 0.50 0.975 \}$ so that for each grid point at 10° of longitude, and 10°, 40°, and 70° of latitude,  the three models were developed to estimate the upper bound (UB), median VTEC (med) and lower bound (UB) with a confidence interval of $95\%$.
"""

qgb_UB_10E70N = joblib.load('qgb_10E70N_VTEC_upper_1d.sav')
qgb_med_10E70N = joblib.load('qgb_10E70N_VTEC_median_1d.sav')
qgb_LB_10E70N = joblib.load('qgb_10E70N_VTEC_lower_1d.sav')

qgb_UB_10E40N = joblib.load('qgb_10E40N_VTEC_upper_1d.sav')
qgb_med_10E40N = joblib.load('qgb_10E40N_VTEC_median_1d.sav')
qgb_LB_10E40N = joblib.load('qgb_10E40N_VTEC_lower_1d.sav')

qgb_UB_10E10N = joblib.load('qgb_10E10N_VTEC_upper_1d.sav')
qgb_med_10E10N = joblib.load('qgb_10E10N_VTEC_median_1d.sav')
qgb_LB_10E10N = joblib.load('qgb_10E10N_VTEC_lower_1d.sav')

y_10E70N_upper = qgb_UB_10E70N.predict(X_70_test_new)
y_10E70N_median = qgb_med_10E70N.predict(X_70_test_new)
y_10E70N_lower = qgb_LB_10E70N.predict(X_70_test_new)

y_10E40N_upper = qgb_UB_10E40N.predict(X_40_test_new)
y_10E40N_median = qgb_med_10E40N.predict(X_40_test_new)
y_10E40N_lower = qgb_LB_10E40N.predict(X_40_test_new)

y_10E10N_upper = qgb_UB_10E10N.predict(X_10_test_new)
y_10E10N_median = qgb_med_10E10N.predict(X_10_test_new)
y_10E10N_lower = qgb_LB_10E10N.predict(X_10_test_new)

"""## **Test data, statistics**"""

print('10E 70N (year 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_70_test_new, y_10E70N_median)),2))
print('Corr. median:',  scipy.stats.pearsonr(y_70_test_new, y_10E70N_median)[0])
print('CI upper:', round((y_10E70N_upper-y_10E70N_median).mean(axis=0), 2))
print('CI lower:', round((y_10E70N_median-y_10E70N_lower).mean(axis=0), 2))
print('CI average:', round((((y_10E70N_upper-y_10E70N_median)+(y_10E70N_median-y_10E70N_lower)).mean(axis=0))/2, 2))

print('10E 70N (25-29 Apr 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_70_test_new[2712:2832], y_10E70N_median[2712:2832])),2))
print('Corr. median:',  scipy.stats.pearsonr(y_70_test_new[2712:2832], y_10E70N_median[2712:2832])[0])
print('CI upper:', round((y_10E70N_upper[2712:2832]-y_10E70N_median[2712:2832]).mean(axis=0), 2))
print('CI lower:', round((y_10E70N_median[2712:2832]-y_10E70N_lower[2712:2832]).mean(axis=0), 2))
print('CI average:', round((((y_10E70N_upper[2712:2832]-y_10E70N_median[2712:2832])+(y_10E70N_median[2712:2832]-y_10E70N_lower[2712:2832])).mean(axis=0))/2, 2))

print('10E 70N (Sept 6-10, 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_70_test_new[5927:6047], y_10E70N_median[5927:6047])),2))
print('Corr. median:',  scipy.stats.pearsonr(y_70_test_new[5927:6047], y_10E70N_median[5927:6047])[0])
print('CI upper:', round((y_10E70N_upper[5927:6047]-y_10E70N_median[5927:6047]).mean(axis=0), 2))
print('CI lower:', round((y_10E70N_median[5927:6047]-y_10E70N_lower[5927:6047]).mean(axis=0), 2))
print('CI average:', round((((y_10E70N_upper[5927:6047]-y_10E70N_median[5927:6047])+(y_10E70N_median[5927:6047]-y_10E70N_lower[5927:6047])).mean(axis=0))/2, 2))

print('10E 40N (year 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_40_test_new, y_10E40N_median)),2))
print('Corr. median:',  scipy.stats.pearsonr(y_40_test_new, y_10E40N_median)[0])
print('CI upper:', round((y_10E40N_upper-y_10E40N_median).mean(axis=0), 2))
print('CI lower:', round((y_10E40N_median-y_10E40N_lower).mean(axis=0), 2))
print('CI average:', round((((y_10E40N_upper-y_10E40N_median)+(y_10E40N_median-y_10E40N_lower)).mean(axis=0))/2, 2))

print('10E 40N (25-29 Apr 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_40_test_new[2712:2832], y_10E40N_median[2712:2832])),2))
print('Corr. median:',  scipy.stats.pearsonr(y_40_test_new[2712:2832], y_10E40N_median[2712:2832])[0])
print('CI upper:', round((y_10E40N_upper[2712:2832]-y_10E40N_median[2712:2832]).mean(axis=0), 2))
print('CI lower:', round((y_10E40N_median[2712:2832]-y_10E40N_lower[2712:2832]).mean(axis=0), 2))
print('CI average:', round((((y_10E40N_upper[2712:2832]-y_10E40N_median[2712:2832])+(y_10E40N_median[2712:2832]-y_10E40N_lower[2712:2832])).mean(axis=0))/2, 2))

print('10E 40N (Sept 6-10, 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_40_test_new[5927:6047], y_10E40N_median[5927:6047])),2))
print('Corr. median:',  scipy.stats.pearsonr(y_40_test_new[5927:6047], y_10E40N_median[5927:6047])[0])
print('CI upper:', round((y_10E40N_upper[5927:6047]-y_10E40N_median[5927:6047]).mean(axis=0), 2))
print('CI lower:', round((y_10E40N_median[5927:6047]-y_10E40N_lower[5927:6047]).mean(axis=0), 2))
print('CI average:', round((((y_10E40N_upper[5927:6047]-y_10E40N_median[5927:6047])+(y_10E40N_median[5927:6047]-y_10E40N_lower[5927:6047])).mean(axis=0))/2, 2))

print('10E 10N (year 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_10_test_new, y_10E10N_median)),2))
print('Corr. median:',  scipy.stats.pearsonr(y_10_test_new, y_10E10N_median)[0])
print('CI upper:', round((y_10E10N_upper-y_10E10N_median).mean(axis=0), 2))
print('CI lower:', round((y_10E10N_median-y_10E10N_lower).mean(axis=0), 2))
print('CI average:', round((((y_10E10N_upper-y_10E10N_median)+(y_10E10N_median-y_10E10N_lower)).mean(axis=0))/2, 2))

print('10E 10N (25-29 Apr 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_10_test_new[2712:2832], y_10E10N_median[2712:2832])),2))
print('Corr. median:',  scipy.stats.pearsonr(y_10_test_new[2712:2832], y_10E10N_median[2712:2832])[0])
print('CI upper:', round((y_10E10N_upper[2712:2832]-y_10E10N_median[2712:2832]).mean(axis=0), 2))
print('CI lower:', round((y_10E10N_median[2712:2832]-y_10E10N_lower[2712:2832]).mean(axis=0), 2))
print('CI average:', round((((y_10E10N_upper[2712:2832]-y_10E10N_median[2712:2832])+(y_10E10N_median[2712:2832]-y_10E10N_lower[2712:2832])).mean(axis=0))/2, 2))

print('10E 10N (Sept 6-10, 2017)' )
print('RMS median:', round(np.sqrt(metrics.mean_squared_error(y_10_test_new[5927:6047], y_10E10N_median[5927:6047])),2))
print('Corr. median:',  scipy.stats.pearsonr(y_10_test_new[5927:6047], y_10E10N_median[5927:6047])[0])
print('CI upper:', round((y_10E10N_upper[5927:6047]-y_10E10N_median[5927:6047]).mean(axis=0), 2))
print('CI lower:', round((y_10E10N_median[5927:6047]-y_10E10N_lower[5927:6047]).mean(axis=0), 2))
print('CI average:', round((((y_10E10N_upper[5927:6047]-y_10E10N_median[5927:6047])+(y_10E10N_median[5927:6047]-y_10E10N_lower[5927:6047])).mean(axis=0))/2, 2))

"""## **Percentage of actual values within confidence intervals (CI)**"""

#2017
diff_up_10 = y_10E10N_upper - y_10_test_new
diff_low_10 = y_10E10N_lower - y_10_test_new
diff_up_40 =y_10E40N_upper - y_40_test_new
diff_low_40 = y_10E40N_lower - y_40_test_new
diff_up_70 = y_10E70N_upper - y_70_test_new
diff_low_70 = y_10E70N_lower - y_70_test_new

diff_up_70.size

a_10=0; b_10=0
a_40=0; b_40=0
a_70=0; b_70=0

for i in range(8758):
  if np.any((diff_up_10[i] >= 0) & (diff_low_10[i] <= 0)):
    a_10=a_10+1
  else:
    b_10=b_10+1
  if np.any((diff_up_40[i] >= 0) & (diff_low_40[i] <= 0)):
    a_40=a_40+1
  else:
    b_40=b_40+1
  if np.any((diff_up_70[i] >= 0) & (diff_low_70[i] <= 0)):
    a_70=a_70+1
  else:
    b_70=b_70+1

whole_10 = a_10 + b_10
whole_40 = a_40 + b_40
whole_70 = a_70 + b_70

percent_in_CI_10 = a_10/whole_10 * 100
percent_in_CI_40 = a_40/whole_40 * 100
percent_in_CI_70 = a_70/whole_70 * 100

print('Percent in CI, 10E 10N, 2017 :', round(percent_in_CI_10,2))
print('Percent in CI, 10E 40N, 2017 :', round(percent_in_CI_40,2))
print('Percent in CI, 10E 70N, 2017 :', round(percent_in_CI_70,2))

#April 25-29, 2017
diff_up_10_apr = y_10E10N_upper[2712:2832] - y_10_test_new[2712:2832]
diff_low_10_apr = y_10E10N_lower[2712:2832] - y_10_test_new[2712:2832]
diff_up_40_apr =y_10E40N_upper[2712:2832] - y_40_test_new[2712:2832]
diff_low_40_apr = y_10E40N_lower[2712:2832] - y_40_test_new[2712:2832]
diff_up_70_apr = y_10E70N_upper[2712:2832] - y_70_test_new[2712:2832]
diff_low_70_apr = y_10E70N_lower[2712:2832] - y_70_test_new[2712:2832]

diff_up_70_apr.size

a_10=0; b_10=0
a_40=0; b_40=0
a_70=0; b_70=0

for i in range(120):
  if np.any((diff_up_10_apr[i] >= 0) & (diff_low_10_apr[i] <= 0)):
    a_10=a_10+1
  else:
    b_10=b_10+1
  if np.any((diff_up_40_apr[i] >= 0) & (diff_low_40_apr[i] <= 0)):
    a_40=a_40+1
  else:
    b_40=b_40+1
  if np.any((diff_up_70_apr[i] >= 0) & (diff_low_70_apr[i] <= 0)):
    a_70=a_70+1
  else:
    b_70=b_70+1

whole_10 = a_10 + b_10
whole_40 = a_40 + b_40
whole_70 = a_70 + b_70

percent_in_CI_10 = a_10/whole_10 * 100
percent_in_CI_40 = a_40/whole_40 * 100
percent_in_CI_70 = a_70/whole_70 * 100

print('Percent in CI, 10E 10N, Apr 25-29, 2017 :', round(percent_in_CI_10,2))
print('Percent in CI, 10E 40N, Apr 25-29, 2017 :', round(percent_in_CI_40,2))
print('Percent in CI, 10E 70N, Apr 25-29, 2017 :', round(percent_in_CI_70,2))

#September 6-10, 2017
diff_up_10_sep = y_10E10N_upper[5927:6047] - y_10_test_new[5927:6047]
diff_low_10_sep = y_10E10N_lower[5927:6047] - y_10_test_new[5927:6047]
diff_up_40_sep =y_10E40N_upper[5927:6047] - y_40_test_new[5927:6047]
diff_low_40_sep = y_10E40N_lower[5927:6047] - y_40_test_new[5927:6047]
diff_up_70_sep = y_10E70N_upper[5927:6047] - y_70_test_new[5927:6047]
diff_low_70_sep = y_10E70N_lower[5927:6047] - y_70_test_new[5927:6047]

diff_up_70_sep.size

a_10=0; b_10=0
a_40=0; b_40=0
a_70=0; b_70=0

for i in range(120):
  if np.any((diff_up_10_sep[i] >= 0) & (diff_low_10_sep[i] <= 0)):
    a_10=a_10+1
  else:
    b_10=b_10+1
  if np.any((diff_up_40_sep[i] >= 0) & (diff_low_40_sep[i] <= 0)):
    a_40=a_40+1
  else:
    b_40=b_40+1
  if np.any((diff_up_70_sep[i] >= 0) & (diff_low_70_sep[i] <= 0)):
    a_70=a_70+1
  else:
    b_70=b_70+1

whole_10 = a_10 + b_10
whole_40 = a_40 + b_40
whole_70 = a_70 + b_70

percent_in_CI_10 = a_10/whole_10 * 100
percent_in_CI_40 = a_40/whole_40 * 100
percent_in_CI_70 = a_70/whole_70 * 100

print('Percent in CI, 10E 10N, Sep 6-10, 2017 :', round(percent_in_CI_10,2))
print('Percent in CI, 10E 40N, Sep 6-10, 2017 :', round(percent_in_CI_40,2))
print('Percent in CI, 10E 70N, Sep 6-10, 2017 :', round(percent_in_CI_70,2))

"""## **Plots**"""

base = datetime.datetime(2017, 9, 6)
dates2 = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates2, y_10E70N_median[5927:6047], 'bo',linewidth=0,markersize=2,  label='VTEC forecast (QGB)')
axs[0].fill_between(dates2, y_10E70N_lower[5927:6047], y_10E70N_upper[5927:6047],  alpha=0.4, color='tab:green',  label='95% Confidence intervals')
axs[0].plot(dates2, y_70_test_new[5927:6047],  'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 21.0, 10.0)))

axs[1].plot(dates2, y_10E40N_median[5927:6047], 'bo',linewidth=0,markersize=2,  label='VTEC forecast (QGB)')
axs[1].fill_between(dates2, y_10E40N_lower[5927:6047], y_10E40N_upper[5927:6047],  alpha=0.4, color='tab:green',  label='95% Confidence intervals')
axs[1].plot(dates2, y_40_test_new[5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 31.0, 10.0)))

axs[2].plot(dates2, y_10E10N_median[5927:6047], 'bo',linewidth=0,markersize=2,  label='VTEC forecast (QGB)')
axs[2].fill_between(dates2, y_10E10N_lower[5927:6047], y_10E10N_upper[5927:6047],  alpha=0.4, color='tab:green',  label='95% Confidence intervals')
axs[2].plot(dates2, y_10_test_new[5927:6047],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 51.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.2, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

base = datetime.datetime(2017, 4, 25)
dates3 = np.array([base + datetime.timedelta(hours=(1 * j))
                  for j in range(120)])

fig, axs = plt.subplots(3, sharex=True)

axs[0].plot(dates3, y_10E70N_median[2712:2832], 'bo',linewidth=0,markersize=2,  label='VTEC forecast (QGB)')
axs[0].fill_between(dates3, y_10E70N_lower[2712:2832], y_10E70N_upper[2712:2832],  alpha=0.4, color='tab:green',  label='95% Confidence intervals')
axs[0].plot(dates3, y_70_test_new[2712:2832],  'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[0].set_title('10E 70N', y=0.75, loc='right')
axs[0].set_yticks((np.arange(0.0, 21.0, 10.0)))

axs[1].plot(dates3, y_10E40N_median[2712:2832], 'bo',linewidth=0,markersize=2,  label='VTEC forecast (QGB)')
axs[1].fill_between(dates3, y_10E40N_lower[2712:2832], y_10E40N_upper[2712:2832],  alpha=0.4, color='tab:green',  label='95% Confidence intervals')
axs[1].plot(dates3, y_40_test_new[2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[1].set_title('10E 40N', y=0.75, loc='right')
axs[1].set(ylabel='VTEC (TECU)')
axs[1].set_yticks((np.arange(0.0, 31.0, 10.0)))

axs[2].plot(dates3, y_10E10N_median[2712:2832], 'bo',linewidth=0,markersize=2,  label='VTEC forecast (QGB)')
axs[2].fill_between(dates3, y_10E10N_lower[2712:2832], y_10E10N_upper[2712:2832],  alpha=0.4, color='tab:green',  label='95% Confidence intervals')
axs[2].plot(dates3, y_10_test_new[2712:2832],   'o',color='tab:orange', linewidth=1,markersize=2, label='GT (GIM CODE)')
axs[2].set_title('10E 10N', y=0.75, loc='right')
axs[2].set_yticks((np.arange(0.0, 51.0, 25.0)))
axs[2].set(xlabel='Time (UTC)')

axs[0].legend(loc="best", bbox_to_anchor=(0.2, 1, 0, 0))
date_form = DateFormatter("%m-%d")
axs[2].xaxis.set_major_formatter(date_form)

plt.rcParams ['figure.figsize'] = [7.0, 6.0]
plt.show()
plt.rcParams.update({'font.size': 15})

"""## **Feature importance**"""

features_70 = X_70_test_df.columns[:]
features_40 = X_40_test_df.columns[:]
features_10 = X_10_test_df.columns[:]

importance = qgb_UB_10E70N.feature_importances_
  plt.barh([x_70 for x_70 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 70N, UB')
  plt.yticks(range(len(features_70)), features_70)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_UB_10E40N.feature_importances_
  plt.barh([x_40 for x_40 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 40N, UB')
  plt.yticks(range(len(features_40)), features_40)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_UB_10E10N.feature_importances_
  plt.barh([x_10 for x_10 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 10N, UB')
  plt.yticks(range(len(features_10)), features_10)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_med_10E70N.feature_importances_
  plt.barh([x_70 for x_70 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 70N, Median')
  plt.yticks(range(len(features_70)), features_70)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_med_10E40N.feature_importances_
  plt.barh([x_40 for x_40 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 40N, Median')
  plt.yticks(range(len(features_40)), features_40)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_med_10E10N.feature_importances_
  plt.barh([x_10 for x_10 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 10N, Median')
  plt.yticks(range(len(features_10)), features_10)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_LB_10E70N.feature_importances_
  plt.barh([x_70 for x_70 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 70N, LB')
  plt.yticks(range(len(features_70)), features_70)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_LB_10E40N.feature_importances_
  plt.barh([x_40 for x_40 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 40N, LB')
  plt.yticks(range(len(features_40)), features_40)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()

importance = qgb_LB_10E10N.feature_importances_
  plt.barh([x_10 for x_10 in range(len(importance))], importance, color='#8f63f4', align='center')
  plt.title('10E 10N, LB')
  plt.yticks(range(len(features_10)), features_10)
  plt.xticks((np.arange(0.0, 1.1, 0.2)))
  plt.rcParams ['figure.figsize'] = [3.6, 5]
  plt.rcParams.update({'font.size': 12})
  plt.xlabel('Relative Importance')
  plt.show()